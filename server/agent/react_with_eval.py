"""
Enhanced ReAct Agent with Evaluation Gating

Implements react_with_eval that gates final answers with evaluator_node before returning.
If the score is below threshold, it attempts one-shot self-repair.
"""
import logging
from typing import Dict, Any, Optional, Protocol
from dataclasses import dataclass

from server.agent.state import State, make_state, Budgets, AgentStatus
from server.agent.react_agent import ReActAgent
from server.agent.evaluator_node import EvaluatorNode, AcceptanceCriteria, get_evaluator_node


logger = logging.getLogger(__name__)


class LLMClient(Protocol):
    """Protocol for LLM client interface"""
    async def complete(self, prompt: str) -> str:
        ...


@dataclass
class EvaluatedResponse:
    """Response from react_with_eval with evaluation results"""
    answer: str
    state: State
    score_after: float
    evaluation_attempted: bool = False
    repair_attempted: bool = False
    original_score: Optional[float] = None
    evaluation_feedback: Optional[str] = None


async def react_with_eval(
    goal: str, 
    budgets: Budgets, 
    acceptance: AcceptanceCriteria,
    llm_client: LLMClient,
    user_id: str = "default_user",
    api_base: str = None,
    model_name: str = None
) -> EvaluatedResponse:
    """
    Enhanced ReAct agent that gates final answers with evaluation.
    
    After the loop ends, runs evaluator_node to score output vs acceptance criteria.
    If below threshold, appends a one-shot self-repair prompt and re-answers once.
    
    Args:
        goal: The goal to achieve
        budgets: Token and iteration budgets
        acceptance: Acceptance criteria for evaluation
        llm_client: LLM client for evaluation and repair
        user_id: User identifier
        api_base: API base URL for the ReAct agent
        model_name: Model name for the ReAct agent
        
    Returns:
        EvaluatedResponse with answer, state, and evaluation results
    """
    logger.info(f"Starting react_with_eval for goal: {goal[:100]}...")
    
    # Step 1: Create and run ReAct agent
    logger.info("Creating ReAct agent...")
    
    # Use default configuration if not provided
    api_base = api_base or "http://localhost:8000/v1"
    model_name = model_name or "gpt-4"
    
    react_agent = ReActAgent(
        api_base=api_base,
        model_name=model_name,
        user_id=user_id,
        default_budgets=budgets
    )
    
    # Run the ReAct agent (this returns a State object)
    logger.info("Running ReAct agent...")
    state = react_agent.react_run_with_state(goal, budgets)
    
    # Extract the final answer from the state
    final_answer = extract_final_answer_from_state(state)
    
    if not final_answer:
        logger.warning("No final answer found in ReAct state")
        return EvaluatedResponse(
            answer="No answer was generated by the ReAct agent.",
            state=state,
            score_after=0.0,
            evaluation_attempted=False
        )
    
    # Step 2: Evaluate the answer
    logger.info("Evaluating answer against acceptance criteria...")
    evaluator = get_evaluator_node(llm_client)
    
    try:
        evaluation = await evaluator.evaluate(final_answer, state, acceptance)
        logger.info(f"Evaluation complete. Score: {evaluation.overall_score:.2f}, Threshold: {acceptance.min_score}")
        
        # Step 3: Check if repair is needed
        if evaluation.overall_score < acceptance.min_score:
            logger.info("Score below threshold, attempting self-repair...")
            
            # Attempt one-shot repair
            try:
                repaired_answer = await evaluator.repair_answer(final_answer, state, evaluation)
                
                # Re-evaluate the repaired answer
                repair_evaluation = await evaluator.evaluate(repaired_answer, state, acceptance)
                logger.info(f"Repair complete. New score: {repair_evaluation.overall_score:.2f}")
                
                return EvaluatedResponse(
                    answer=repaired_answer,
                    state=state,
                    score_after=repair_evaluation.overall_score,
                    evaluation_attempted=True,
                    repair_attempted=True,
                    original_score=evaluation.overall_score,
                    evaluation_feedback=repair_evaluation.feedback
                )
                
            except Exception as e:
                logger.error(f"Error during repair attempt: {e}")
                # Return original answer if repair fails
                return EvaluatedResponse(
                    answer=final_answer,
                    state=state,
                    score_after=evaluation.overall_score,
                    evaluation_attempted=True,
                    repair_attempted=False,
                    original_score=evaluation.overall_score,
                    evaluation_feedback=evaluation.feedback
                )
        else:
            # Score meets threshold, return original answer
            logger.info("Score meets threshold, returning original answer")
            return EvaluatedResponse(
                answer=final_answer,
                state=state,
                score_after=evaluation.overall_score,
                evaluation_attempted=True,
                repair_attempted=False,
                evaluation_feedback=evaluation.feedback
            )
            
    except Exception as e:
        logger.error(f"Error during evaluation: {e}")
        # Return answer without evaluation if evaluation fails
        return EvaluatedResponse(
            answer=final_answer,
            state=state,
            score_after=0.5,  # Neutral score when evaluation fails
            evaluation_attempted=False,
            evaluation_feedback=f"Evaluation failed: {str(e)}"
        )


def extract_final_answer_from_state(state: State) -> str:
    """
    Extract the final answer from ReAct agent state.
    
    Args:
        state: State object from ReAct agent
        
    Returns:
        The final answer string
    """
    # Check if there's a final_response field
    if hasattr(state, 'final_response') and state.final_response:
        return state.final_response
    
    # Look in state.context for final answer patterns
    if hasattr(state, 'context') and state.context:
        context = state.context
        
        # Look for "Final Answer:" pattern
        import re
        final_answer_match = re.search(r"Final Answer:\s*(.+?)(?:\n|$)", context, re.IGNORECASE | re.DOTALL)
        if final_answer_match:
            return final_answer_match.group(1).strip()
        
        # Look for last assistant message if formatted as conversation
        assistant_messages = re.findall(r"Assistant:\s*(.+?)(?=\nUser:|$)", context, re.IGNORECASE | re.DOTALL)
        if assistant_messages:
            return assistant_messages[-1].strip()
    
    # Check evidence for final answer
    if hasattr(state, 'evidence') and state.evidence:
        evidence_entries = state.evidence.entries
        if evidence_entries:
            # Get the last evidence entry as potential final answer
            last_entry = evidence_entries[-1]
            if hasattr(last_entry, 'content'):
                return last_entry.content
    
    # Fallback: return string representation of state context
    logger.warning("Could not extract clear final answer from ReAct state")
    return getattr(state, 'context', 'No answer found')


# Convenience function for common use cases
async def react_with_basic_eval(
    goal: str,
    llm_client: LLMClient,
    min_score: float = 0.7,
    required_elements: list[str] = None,
    prohibited_elements: list[str] = None,
    user_id: str = "default_user"
) -> EvaluatedResponse:
    """
    Convenience function for react_with_eval with basic acceptance criteria.
    
    Args:
        goal: The goal to achieve
        llm_client: LLM client for evaluation and repair
        min_score: Minimum score threshold (default 0.7)
        required_elements: List of required elements in the answer
        prohibited_elements: List of prohibited elements
        user_id: User identifier
        
    Returns:
        EvaluatedResponse with evaluation results
    """
    budgets = Budgets()  # Use default budgets
    acceptance = AcceptanceCriteria(
        min_score=min_score,
        required_elements=required_elements or [],
        prohibited_elements=prohibited_elements or []
    )
    
    return await react_with_eval(goal, budgets, acceptance, llm_client, user_id)
