#!/usr/bin/env python3
"""
Demo: Swarm-Style Deterministic Execution

This demo validates the new deterministic execution approach:
1. Replace ReAct looping with explicit steps: prepare_inputs → execute → postprocess
2. Tasks run deterministically with fewer loops
3. Modular, Swarm-style handoffs

Success criteria:
- Tasks execute through explicit functions without ReAct scratchpad
- Execution is deterministic (same inputs = same outputs)
- State machine orchestrates the flow properly
"""
import sys
import os
import asyncio
import logging

# Add the server directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'server'))

from server.agent.task_orchestrator import TaskOrchestrator, OrchestrationResult, OrchestrationStatus
from server.agent.swarm_execution import (
    run_minimal_flow, branch_executor, ExecutionFlow,
    function_prepare_inputs, function_execute, function_postprocess
)
from server.agent.state import make_state, Budgets
from server.agent.evaluator_node import AcceptanceCriteria

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class MockLLMClient:
    """Mock LLM client for testing"""
    
    async def acreate(self, messages, **kwargs):
        """Mock LLM response"""
        
        class MockResponse:
            def __init__(self, content):
                self.choices = [type('Choice', (), {'message': type('Message', (), {'content': content})})()]
        
        # Generate a simple response based on the last user message
        last_message = messages[-1].get('content', '') if messages else ''
        
        if 'score' in last_message.lower():
            return MockResponse("Score: 0.85\nFeedback: Good response with clear structure.")
        else:
            return MockResponse("This is a deterministic response generated by the Swarm execution system.")


async def test_individual_functions():
    """Test the individual Swarm execution functions"""
    
    print("\n=== Testing Individual Swarm Functions ===\n")
    
    # Test 1: prepare_inputs
    print("1. Testing function_prepare_inputs...")
    goal = "Explain binary search algorithm"
    context = ["Some context about algorithms"]
    
    prep_result = function_prepare_inputs(goal, context)
    print(f"   Status: {'SUCCESS' if prep_result.ok else 'FAILED'}")
    if prep_result.ok:
        print(f"   Strategy: {prep_result.output.get('strategy', 'unknown')}")
        print(f"   Required tools: {len(prep_result.output.get('required_tools', []))}")
        print(f"   Cost: {prep_result.cost}")
    else:
        print(f"   Error: {prep_result.error}")
    
    # Test 2: execute
    if prep_result.ok:
        print("\n2. Testing function_execute...")
        exec_result = await function_execute(prep_result.output)
        print(f"   Status: {'SUCCESS' if exec_result.ok else 'FAILED'}")
        if exec_result.ok:
            print(f"   Output length: {len(exec_result.output)}")
            print(f"   Cost: {exec_result.cost}")
        else:
            print(f"   Error: {exec_result.error}")
    
        # Test 3: postprocess
        if exec_result.ok:
            print("\n3. Testing function_postprocess...")
            post_result = await function_postprocess(exec_result.output)
            print(f"   Status: {'SUCCESS' if post_result.ok else 'FAILED'}")
            if post_result.ok:
                print(f"   Formatted output length: {len(post_result.output)}")
                print(f"   Cost: {post_result.cost}")
            else:
                print(f"   Error: {post_result.error}")
    
    return prep_result.ok and exec_result.ok and post_result.ok


async def test_minimal_flow():
    """Test the complete minimal flow execution"""
    
    print("\n=== Testing Complete Minimal Flow ===\n")
    
    # Create test state
    goal = "Implement a binary search algorithm in Python"
    budgets = Budgets()
    state = make_state(goal, budgets)
    state.context = ["Working on algorithms project"]
    
    # Test the flow
    result = await run_minimal_flow(state)
    
    print(f"Flow Status: {'SUCCESS' if result.success else 'FAILED'}")
    print(f"Total Cost: {result.total_cost}")
    print(f"Steps Completed: {len(result.step_results)}")
    
    if result.success:
        print(f"Final Output Length: {len(result.final_output)}")
        print(f"Strategies Used: {result.metadata.get('strategies_used', [])}")
        print(f"Tools Used: {result.metadata.get('tools_used', [])}")
        
        # Show step details
        for i, step in enumerate(result.step_results, 1):
            print(f"  Step {i}: {'SUCCESS' if step.ok else 'FAILED'} (cost: {step.cost})")
    else:
        print(f"Error: {result.error}")
        
    return result.success


async def test_branch_executor():
    """Test the branch executor routing"""
    
    print("\n=== Testing Branch Executor ===\n")
    
    # Create test state with different routes
    goal = "Explain Python decorators"
    budgets = Budgets()
    state = make_state(goal, budgets)
    state.route = "MINIMAL_FLOW"
    
    # Test branch executor
    result = await branch_executor(state)
    
    print(f"Branch Executor Status: {'SUCCESS' if result.success else 'FAILED'}")
    print(f"Route Used: {getattr(state, 'route', 'default')}")
    print(f"Total Cost: {result.total_cost}")
    
    if result.success:
        print(f"Output Preview: {result.final_output[:100]}...")
    else:
        print(f"Error: {result.error}")
        
    return result.success


async def test_orchestrator_integration():
    """Test the complete orchestrator with Swarm execution"""
    
    print("\n=== Testing Orchestrator Integration ===\n")
    
    # Create orchestrator with mock LLM
    mock_llm = MockLLMClient()
    orchestrator = TaskOrchestrator(llm_client=mock_llm)
    
    # Set up task
    goal = "Create a simple sorting algorithm and explain how it works"
    budgets = Budgets(total_tokens=1000, max_iterations=5)
    acceptance = AcceptanceCriteria(min_score=0.7)
    
    # Run task
    result = await orchestrator.run_task(goal, budgets, acceptance)
    
    print(f"Orchestrator Status: {result.status.value}")
    print(f"Success: {result.status == OrchestrationStatus.DONE}")
    
    if result.answer:
        print(f"Answer Length: {len(result.answer)}")
        print(f"Answer Preview: {result.answer[:150]}...")
    
    if result.state:
        print(f"Final Cost: {result.state.cost}")
        print(f"Evidence Count: {len(result.state.evidence)}")
        print(f"Subgoals: {getattr(result.state, 'subgoals', [])}")
    
    if result.metadata:
        print(f"Final Score: {result.metadata.get('final_score', 'N/A')}")
    
    if result.error:
        print(f"Error: {result.error}")
    
    return result.status == OrchestrationStatus.DONE


async def test_deterministic_behavior():
    """Test that execution is deterministic"""
    
    print("\n=== Testing Deterministic Behavior ===\n")
    
    goal = "Explain what is Python"
    budgets = Budgets()
    state1 = make_state(goal, budgets)
    state2 = make_state(goal, budgets)
    
    # Run the same goal twice
    result1 = await run_minimal_flow(state1)
    result2 = await run_minimal_flow(state2)
    
    print(f"Run 1 Status: {'SUCCESS' if result1.success else 'FAILED'}")
    print(f"Run 2 Status: {'SUCCESS' if result2.success else 'FAILED'}")
    
    if result1.success and result2.success:
        # Check for consistency in structure
        same_steps = len(result1.step_results) == len(result2.step_results)
        same_strategies = (result1.metadata.get('strategies_used', []) == 
                          result2.metadata.get('strategies_used', []))
        
        print(f"Same number of steps: {same_steps}")
        print(f"Same strategies used: {same_strategies}")
        
        # Outputs might differ slightly due to mock responses, but structure should be similar
        length_diff = abs(len(result1.final_output) - len(result2.final_output))
        print(f"Output length difference: {length_diff} chars")
        
        deterministic = same_steps and same_strategies and length_diff < 100
        print(f"Deterministic behavior: {deterministic}")
        
        return deterministic
    
    return False


async def main():
    """Run all tests"""
    
    print("Swarm-Style Deterministic Execution Demo")
    print("=" * 50)
    
    tests = [
        ("Individual Functions", test_individual_functions),
        ("Minimal Flow", test_minimal_flow),
        ("Branch Executor", test_branch_executor),
        ("Orchestrator Integration", test_orchestrator_integration),
        ("Deterministic Behavior", test_deterministic_behavior),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n🔄 Running {test_name}...")
        try:
            success = await test_func()
            results.append((test_name, success))
            print(f"✅ {test_name}: {'PASSED' if success else 'FAILED'}")
        except Exception as e:
            results.append((test_name, False))
            print(f"❌ {test_name}: EXCEPTION - {e}")
    
    # Summary
    print(f"\n{'=' * 50}")
    print("SUMMARY:")
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "✅ PASS" if success else "❌ FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\n🎉 SUCCESS: All tests passed!")
        print("✅ Swarm-style deterministic execution is working correctly")
        print("✅ ReAct looping has been replaced with explicit steps")
        print("✅ Tasks run deterministically with modular handoffs")
    else:
        print(f"\n⚠️  {total - passed} test(s) failed. Check implementation.")
    
    return passed == total


if __name__ == "__main__":
    asyncio.run(main())
