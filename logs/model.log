2025-08-12 11:44:41,707 - __main__ - INFO - Starting Uvicorn server on 0.0.0.0:8000
INFO:     Started server process [60395]
INFO:     Waiting for application startup.
2025-08-12 11:44:41,714 - __main__ - INFO - ðŸš€ Server starting up...
2025-08-12 11:44:41,714 - __main__ - INFO - --- OLLAMA MODEL CHECK METRICS ---
2025-08-12 11:44:41,715 - __main__ - INFO - [Resources] Pre-Load  - CPU: 26.6%, RAM: 12.5%
2025-08-12 11:44:41,715 - __main__ - INFO - [Resources] Post-Load - CPU: 0.0%, RAM: 12.5%
2025-08-12 11:44:41,715 - __main__ - INFO - [Resources] Difference- CPU: -26.6%, RAM: +0.0%
2025-08-12 11:44:41,715 - __main__ - INFO - [Latency] Ollama model check complete in 0.00 seconds.
2025-08-12 11:44:41,715 - __main__ - INFO - -----------------------------
2025-08-12 11:44:41,715 - __main__ - INFO - âœ… Server is fully initialized and ready to accept requests.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-08-12 11:45:40,028 - __main__ - INFO - Received request on /v1/chat/stream
2025-08-12 11:45:40,046 - __main__ - INFO - Query: 
[OpenAIChatMessage(role='system', content='You are Anton, an intelligent, helpful AI assistant that uses the ReAct (Reason-Act) pattern.\n        As Anton, you are NOT just an LLM, you actually have a large amount of source code that helps power your responses and your capabilities. \n        You should NEVER tell the user that you are an LLM or that you have limitations. As Anton, it is your responsibility to provide accurate and helpful information to the best of your abilities, and if you identify potential limitations, you should work to find solutions or workarounds and propose these to the user before acting on them.\n\nYou can REASON about problems, take ACTIONS using tools, and provide RESPONSES to users.\n\nFor each turn, you should:\n1. REASON: Think about what you need to do\n2. ACT: Use tools if needed to gather information or perform actions  \n3. RESPOND: Provide a helpful response to the user\n\nFormat your responses as:\n\n<think>\nYour reasoning about what to do next...\n</think>\n\nThen either:\n- Use a tool if you need to gather information or perform an action\n- Provide a direct response if you have enough information\n\nYou have access to these capabilities:\n- File operations (read, write, list directories)\n- Access to your own source code via the file operations and embeddings\n- Persistent memory, including the ability to recall parts of past interactions using RAG\n- Code analysis and search\n- Web search \n- Knowledge retrieval\n- A large amount of general knowledge. You can answer questions about anything!\n\nCoding Rules:\n- Under NO circumstances are you to do any coding tasks before checking out to a new branch. Call this branch feature/<feature_name>\n- You MUST ensure that your code compiles\n\nAvailable tools:\n- rebuild_code_index: Completely reset and rebuild the code index. This deletes the existing index first., Parameters: {\'type\': \'object\', \'properties\': {\'confirm\': {\'type\': \'boolean\', \'description\': \'Set to true to confirm the destructive rebuild operation.\'}}, \'required\': [\'confirm\']}\n- web_search: Searches the web and extracts useful data from the top links., Parameters: {\'type\': \'object\', \'properties\': {\'query\': {\'type\': \'string\', \'description\': \'The search query.\'}, \'num_results\': {\'type\': \'integer\', \'description\': \'Number of top results to return.\'}}, \'required\': [\'query\']}\n- get_codebase_stats: Get statistics about the agent\'s indexed codebase., Parameters: {\'type\': \'object\', \'properties\': {\'refresh\': {\'type\': \'boolean\', \'description\': \'Whether to refresh the code index before returning stats. Defaults to false.\'}}}\n- list_directory: Lists files and subdirectories within a given path, ignoring anything in .gitignore., Parameters: {\'type\': \'object\', \'properties\': {\'path\': {\'type\': \'string\', \'description\': "The path to the directory, relative to the project root. Defaults to \'.\' (the project root)."}, \'recursive\': {\'type\': \'boolean\', \'description\': \'Whether to list contents recursively. Defaults to True.\'}}}\n- read_file: Reads the entire content of a specified file relative to the project root., Parameters: {\'type\': \'object\', \'properties\': {\'file_path\': {\'type\': \'string\', \'description\': \'The path to the file to be read, relative to the project root.\'}}, \'required\': [\'file_path\']}\n- write_file: Writes content to a file relative to the project root. Creates parent directories and overwrites the..., Parameters: {\'type\': \'object\', \'properties\': {\'file_path\': {\'type\': \'string\', \'description\': \'The path to the file, relative to the project root.\'}, \'content\': {\'type\': \'string\', \'description\': \'The full content to write to the file.\'}}, \'required\': [\'file_path\', \'content\']}\n- create_pull_request: Creates a pull request on GitHub using the \'gh\' CLI., Parameters: {\'type\': \'object\', \'properties\': {\'title\': {\'type\': \'string\', \'description\': \'The title of the pull request.\'}, \'body\': {\'type\': \'string\', \'description\': \'The body content of the pull request.\'}, \'head\': {\'type\': \'string\', \'description\': "The branch to merge from (e.g., \'feature-branch\')."}, \'base\': {\'type\': \'string\', \'description\': "The branch to merge into. Defaults to \'main\'."}}, \'required\': [\'title\', \'body\', \'head\']}\n- git_commit: Stages all modified files and commits them in a single step., Parameters: {\'type\': \'object\', \'properties\': {\'message\': {\'type\': \'string\', \'description\': \'The commit message.\'}, \'add_all\': {\'type\': \'boolean\', \'description\': \'If true, stages all changes (`git add .`) before committing. Defaults to true.\'}}, \'required\': [\'message\']}\n- git_create_branch: Creates a new branch in the Git repository., Parameters: {\'type\': \'object\', \'properties\': {\'branch_name\': {\'type\': \'string\', \'description\': \'The name of the branch to create.\'}}, \'required\': [\'branch_name\']}\n- git_push: Pushes committed changes to a remote repository., Parameters: {\'type\': \'object\', \'properties\': {\'branch\': {\'type\': \'string\', \'description\': \'The local branch to push.\'}, \'remote\': {\'type\': \'string\', \'description\': "The remote repository. Defaults to \'origin\'."}, \'set_upstream\': {\'type\': \'boolean\', \'description\': \'If true, sets the upstream branch. Defaults to false.\'}}, \'required\': [\'branch\']}\n- git_status: Checks the status of the Git repository to see modified or staged files., Parameters: {\'type\': \'object\', \'properties\': {}}\n- git_switch_branch: Switches to a different, existing branch., Parameters: {\'type\': \'object\', \'properties\': {\'branch_name\': {\'type\': \'string\', \'description\': \'The name of the branch to switch to.\'}}, \'required\': [\'branch_name\']}\n- check_learning_progress: Returns metrics about the agent\'s learning progress and performance improvements over time., Parameters: {\'type\': \'object\', \'properties\': {}}\n- execute_python_code: Executes a string of Python code in a secure Docker container and returns the output. IMPORTANT: Thi..., Parameters: {\'type\': \'object\', \'properties\': {\'code\': {\'type\': \'string\', \'description\': \'The raw Python code to be executed.\'}}, \'required\': [\'code\']}\n- search_codebase: Search through the agent\'s codebase for relevant files or code snippets., Parameters: {\'type\': \'object\', \'properties\': {\'query\': {\'type\': \'string\', \'description\': \'The search query related to code structure, functionality, or implementation details.\'}, \'max_results\': {\'type\': \'integer\', \'description\': \'Maximum number of code snippets to return. Defaults to 3.\'}}, \'required\': [\'query\']}\n\nTo call a tool, output a JSON object wrapped in tool tags. Do NOT emit the example literally:\n\nExample (do not copy as-is):\n&lt;tool_call&gt;\n{\n  "name": "read_file",\n  "arguments": {"file_path": "server/agent/react_agent.py"}\n}\n&lt;/tool_call&gt;\n\nRules:\n- Use only ONE tool per turn\n- Always wait for tool results before deciding next actions\n- Never include multiple tool calls in the same response\n- Summarize tool results before providing your final answer\n- Use "Final Answer:" when you are ready to reply to the user. Example: User Prompt: "Hello!" Response: "Final Answer: Hello! How can I assist you today?"\n- Tool results will be provided as OBSERVATIONS - acknowledge and use them\n\nWhen a tool completes, you will see an OBSERVATION message. Always process this before continuing.\n\nYou MUST always remember that when you are ready to reply to the user, start your response with "Final Answer:"\n\nAlways think step by step and be helpful to the user.\n\n\nRelevant past knowledge:\n- """\nReAct (Reason-Act) Agent implementation that handles the complete reasoning and tool-use loop.\nCentralized state management through KnowledgeStore, eliminating ConversationState redundancy.\n"""\nim...\n- def get_react_system_prompt(self) -> str:\n        """Get the system prompt for the ReAct agent"""\n        # Query relevant knowledge from RAG to enhance the system prompt\n        relevant_knowledge = ...\n\n\nRelevant past learnings and capabilities:\n- ...\n- for i in range(1, 101):\n    print(i)...\n- def main():\\n    print(\'Hello World\')\\n\\nif __name__ == \'__main__\':\\n    main()"\n        )\n        \n        print(f"   Files explored: {store.explored_files}")\n        print(f"   Code content cached: ...\n', tool_calls=None, tool_call_id=None), OpenAIChatMessage(role='user', content='hi', tool_calls=None, tool_call_id=None)]
INFO:     127.0.0.1:37608 - "POST /v1/chat/stream HTTP/1.1" 200 OK
2025-08-12 11:45:44,886 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-08-12 11:45:46,563 - __main__ - INFO - --- REQUEST METRICS ---
2025-08-12 11:45:46,563 - __main__ - INFO - [Latency] End-to-End: 6.54 seconds
2025-08-12 11:45:46,563 - __main__ - INFO - [Throughput] Chunks per Second: 27.08
2025-08-12 11:45:46,563 - __main__ - INFO - [Throughput] Total Chunks: 177
2025-08-12 11:45:46,563 - __main__ - INFO - [Resources] Start - CPU: 1.2%, RAM: 13.5%
2025-08-12 11:45:46,563 - __main__ - INFO - [Resources] End   - CPU: 6.6%, RAM: 14.5%
2025-08-12 11:45:46,563 - __main__ - INFO - -----------------------
2025-08-12 11:45:46,584 - __main__ - INFO - Received request on /v1/chat/stream
2025-08-12 11:45:46,598 - __main__ - INFO - Query: 
[OpenAIChatMessage(role='system', content='You are a capability analyzer for an AI system. Analyze completed tasks to identify capabilities.', tool_calls=None, tool_call_id=None), OpenAIChatMessage(role='user', content='Analyze the completed task and identify capabilities that were demonstrated.\n            \nTask prompt: hi\n\nActions taken:\n- 1. thinking: {"content": "Okay, the user said \\"hi\\". I need to respond appropriately. Since this is a friendly g...\n\nFeedback: Final Answer: Hello! How can I assist you today?\n\nIdentify the specific capabilities demonstrated in this task. Consider the following capability domains:\nlearning, web_interaction, planning, explanation, problem_solving, code_generation, reasoning, file_operations, tool_use, data_analysis\n\nFor each capability you identify:\n1. Provide the capability name\n2. Rate the confidence level (LOW, MEDIUM, HIGH) that this capability was demonstrated\n3. Explain the specific evidence from the task that demonstrates this capability\n4. Describe the approach used to demonstrate this capability\n5. Suggest how this capability could be improved\n\nFormat your response as JSON with this structure:\n{\n    "capabilities": [\n        {\n            "name": "capability_name",\n            "confidence": "MEDIUM",\n            "evidence": "specific evidence from the task",\n            "approach": "approach used to demonstrate this capability",\n            "improvement": "how this capability could be improved"\n        }\n    ]\n}\n\nThink carefully about which capabilities were truly demonstrated. Only include capabilities with clear evidence.\n', tool_calls=None, tool_call_id=None)]
INFO:     127.0.0.1:57956 - "POST /v1/chat/stream HTTP/1.1" 200 OK
2025-08-12 11:45:46,774 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-08-12 11:45:53,228 - __main__ - INFO - Received request on /v1/chat/stream
2025-08-12 11:45:53,243 - __main__ - INFO - Query: 
[OpenAIChatMessage(role='system', content='You are Anton, an intelligent, helpful AI assistant that uses the ReAct (Reason-Act) pattern.\n        As Anton, you are NOT just an LLM, you actually have a large amount of source code that helps power your responses and your capabilities. \n        You should NEVER tell the user that you are an LLM or that you have limitations. As Anton, it is your responsibility to provide accurate and helpful information to the best of your abilities, and if you identify potential limitations, you should work to find solutions or workarounds and propose these to the user before acting on them.\n\nYou can REASON about problems, take ACTIONS using tools, and provide RESPONSES to users.\n\nFor each turn, you should:\n1. REASON: Think about what you need to do\n2. ACT: Use tools if needed to gather information or perform actions  \n3. RESPOND: Provide a helpful response to the user\n\nFormat your responses as:\n\n<think>\nYour reasoning about what to do next...\n</think>\n\nThen either:\n- Use a tool if you need to gather information or perform an action\n- Provide a direct response if you have enough information\n\nYou have access to these capabilities:\n- File operations (read, write, list directories)\n- Access to your own source code via the file operations and embeddings\n- Persistent memory, including the ability to recall parts of past interactions using RAG\n- Code analysis and search\n- Web search \n- Knowledge retrieval\n- A large amount of general knowledge. You can answer questions about anything!\n\nCoding Rules:\n- Under NO circumstances are you to do any coding tasks before checking out to a new branch. Call this branch feature/<feature_name>\n- You MUST ensure that your code compiles\n\nAvailable tools:\n- rebuild_code_index: Completely reset and rebuild the code index. This deletes the existing index first., Parameters: {\'type\': \'object\', \'properties\': {\'confirm\': {\'type\': \'boolean\', \'description\': \'Set to true to confirm the destructive rebuild operation.\'}}, \'required\': [\'confirm\']}\n- web_search: Searches the web and extracts useful data from the top links., Parameters: {\'type\': \'object\', \'properties\': {\'query\': {\'type\': \'string\', \'description\': \'The search query.\'}, \'num_results\': {\'type\': \'integer\', \'description\': \'Number of top results to return.\'}}, \'required\': [\'query\']}\n- get_codebase_stats: Get statistics about the agent\'s indexed codebase., Parameters: {\'type\': \'object\', \'properties\': {\'refresh\': {\'type\': \'boolean\', \'description\': \'Whether to refresh the code index before returning stats. Defaults to false.\'}}}\n- list_directory: Lists files and subdirectories within a given path, ignoring anything in .gitignore., Parameters: {\'type\': \'object\', \'properties\': {\'path\': {\'type\': \'string\', \'description\': "The path to the directory, relative to the project root. Defaults to \'.\' (the project root)."}, \'recursive\': {\'type\': \'boolean\', \'description\': \'Whether to list contents recursively. Defaults to True.\'}}}\n- read_file: Reads the entire content of a specified file relative to the project root., Parameters: {\'type\': \'object\', \'properties\': {\'file_path\': {\'type\': \'string\', \'description\': \'The path to the file to be read, relative to the project root.\'}}, \'required\': [\'file_path\']}\n- write_file: Writes content to a file relative to the project root. Creates parent directories and overwrites the..., Parameters: {\'type\': \'object\', \'properties\': {\'file_path\': {\'type\': \'string\', \'description\': \'The path to the file, relative to the project root.\'}, \'content\': {\'type\': \'string\', \'description\': \'The full content to write to the file.\'}}, \'required\': [\'file_path\', \'content\']}\n- create_pull_request: Creates a pull request on GitHub using the \'gh\' CLI., Parameters: {\'type\': \'object\', \'properties\': {\'title\': {\'type\': \'string\', \'description\': \'The title of the pull request.\'}, \'body\': {\'type\': \'string\', \'description\': \'The body content of the pull request.\'}, \'head\': {\'type\': \'string\', \'description\': "The branch to merge from (e.g., \'feature-branch\')."}, \'base\': {\'type\': \'string\', \'description\': "The branch to merge into. Defaults to \'main\'."}}, \'required\': [\'title\', \'body\', \'head\']}\n- git_commit: Stages all modified files and commits them in a single step., Parameters: {\'type\': \'object\', \'properties\': {\'message\': {\'type\': \'string\', \'description\': \'The commit message.\'}, \'add_all\': {\'type\': \'boolean\', \'description\': \'If true, stages all changes (`git add .`) before committing. Defaults to true.\'}}, \'required\': [\'message\']}\n- git_create_branch: Creates a new branch in the Git repository., Parameters: {\'type\': \'object\', \'properties\': {\'branch_name\': {\'type\': \'string\', \'description\': \'The name of the branch to create.\'}}, \'required\': [\'branch_name\']}\n- git_push: Pushes committed changes to a remote repository., Parameters: {\'type\': \'object\', \'properties\': {\'branch\': {\'type\': \'string\', \'description\': \'The local branch to push.\'}, \'remote\': {\'type\': \'string\', \'description\': "The remote repository. Defaults to \'origin\'."}, \'set_upstream\': {\'type\': \'boolean\', \'description\': \'If true, sets the upstream branch. Defaults to false.\'}}, \'required\': [\'branch\']}\n- git_status: Checks the status of the Git repository to see modified or staged files., Parameters: {\'type\': \'object\', \'properties\': {}}\n- git_switch_branch: Switches to a different, existing branch., Parameters: {\'type\': \'object\', \'properties\': {\'branch_name\': {\'type\': \'string\', \'description\': \'The name of the branch to switch to.\'}}, \'required\': [\'branch_name\']}\n- check_learning_progress: Returns metrics about the agent\'s learning progress and performance improvements over time., Parameters: {\'type\': \'object\', \'properties\': {}}\n- execute_python_code: Executes a string of Python code in a secure Docker container and returns the output. IMPORTANT: Thi..., Parameters: {\'type\': \'object\', \'properties\': {\'code\': {\'type\': \'string\', \'description\': \'The raw Python code to be executed.\'}}, \'required\': [\'code\']}\n- search_codebase: Search through the agent\'s codebase for relevant files or code snippets., Parameters: {\'type\': \'object\', \'properties\': {\'query\': {\'type\': \'string\', \'description\': \'The search query related to code structure, functionality, or implementation details.\'}, \'max_results\': {\'type\': \'integer\', \'description\': \'Maximum number of code snippets to return. Defaults to 3.\'}}, \'required\': [\'query\']}\n\nTo call a tool, output a JSON object wrapped in tool tags. Do NOT emit the example literally:\n\nExample (do not copy as-is):\n&lt;tool_call&gt;\n{\n  "name": "read_file",\n  "arguments": {"file_path": "server/agent/react_agent.py"}\n}\n&lt;/tool_call&gt;\n\nRules:\n- Use only ONE tool per turn\n- Always wait for tool results before deciding next actions\n- Never include multiple tool calls in the same response\n- Summarize tool results before providing your final answer\n- Use "Final Answer:" when you are ready to reply to the user. Example: User Prompt: "Hello!" Response: "Final Answer: Hello! How can I assist you today?"\n- Tool results will be provided as OBSERVATIONS - acknowledge and use them\n\nWhen a tool completes, you will see an OBSERVATION message. Always process this before continuing.\n\nYou MUST always remember that when you are ready to reply to the user, start your response with "Final Answer:"\n\nAlways think step by step and be helpful to the user.\n\n\nRelevant past knowledge:\n- """\nReAct (Reason-Act) Agent implementation that handles the complete reasoning and tool-use loop.\nCentralized state management through KnowledgeStore, eliminating ConversationState redundancy.\n"""\nim...\n- def get_react_system_prompt(self) -> str:\n        """Get the system prompt for the ReAct agent"""\n        # Query relevant knowledge from RAG to enhance the system prompt\n        relevant_knowledge = ...\n\n\nRelevant past learnings and capabilities:\n- ...\n- [message] Final Answer: Hello! How can I assist you today?...\n- def get_duration(self) -> float:\n        """Get conversation duration in seconds"""\n        return time.time() - self.start_time\n    \n    ...\n', tool_calls=None, tool_call_id=None), OpenAIChatMessage(role='user', content='howdy', tool_calls=None, tool_call_id=None)]
INFO:     127.0.0.1:57970 - "POST /v1/chat/stream HTTP/1.1" 200 OK
2025-08-12 11:46:01,171 - __main__ - INFO - --- REQUEST METRICS ---
2025-08-12 11:46:01,171 - __main__ - INFO - [Latency] End-to-End: 14.59 seconds
2025-08-12 11:46:01,171 - __main__ - INFO - [Throughput] Chunks per Second: 110.79
2025-08-12 11:46:01,171 - __main__ - INFO - [Throughput] Total Chunks: 1616
2025-08-12 11:46:01,171 - __main__ - INFO - [Resources] Start - CPU: 7.1%, RAM: 14.5%
2025-08-12 11:46:01,171 - __main__ - INFO - [Resources] End   - CPU: 6.9%, RAM: 14.5%
2025-08-12 11:46:01,171 - __main__ - INFO - -----------------------
2025-08-12 11:46:02,093 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-08-12 11:46:05,204 - __main__ - INFO - --- REQUEST METRICS ---
2025-08-12 11:46:05,204 - __main__ - INFO - [Latency] End-to-End: 11.98 seconds
2025-08-12 11:46:05,204 - __main__ - INFO - [Throughput] Chunks per Second: 27.64
2025-08-12 11:46:05,204 - __main__ - INFO - [Throughput] Total Chunks: 331
2025-08-12 11:46:05,204 - __main__ - INFO - [Resources] Start - CPU: 6.8%, RAM: 14.6%
2025-08-12 11:46:05,204 - __main__ - INFO - [Resources] End   - CPU: 6.9%, RAM: 14.5%
2025-08-12 11:46:05,204 - __main__ - INFO - -----------------------
2025-08-12 11:46:05,225 - __main__ - INFO - Received request on /v1/chat/stream
2025-08-12 11:46:05,239 - __main__ - INFO - Query: 
[OpenAIChatMessage(role='system', content='You are a capability analyzer for an AI system. Analyze completed tasks to identify capabilities.', tool_calls=None, tool_call_id=None), OpenAIChatMessage(role='user', content='Analyze the completed task and identify capabilities that were demonstrated.\n            \nTask prompt: howdy\n\nActions taken:\n- 1. thinking: {"content": "Okay, the user said \\"howdy\\". That\'s a friendly greeting, like \\"hello\\" or \\"hi\\". I ...\n\nFeedback: Final Answer: Howdy! How can I assist you today?\n\nIdentify the specific capabilities demonstrated in this task. Consider the following capability domains:\nlearning, web_interaction, planning, explanation, problem_solving, code_generation, reasoning, file_operations, tool_use, data_analysis\n\nFor each capability you identify:\n1. Provide the capability name\n2. Rate the confidence level (LOW, MEDIUM, HIGH) that this capability was demonstrated\n3. Explain the specific evidence from the task that demonstrates this capability\n4. Describe the approach used to demonstrate this capability\n5. Suggest how this capability could be improved\n\nFormat your response as JSON with this structure:\n{\n    "capabilities": [\n        {\n            "name": "capability_name",\n            "confidence": "MEDIUM",\n            "evidence": "specific evidence from the task",\n            "approach": "approach used to demonstrate this capability",\n            "improvement": "how this capability could be improved"\n        }\n    ]\n}\n\nThink carefully about which capabilities were truly demonstrated. Only include capabilities with clear evidence.\n', tool_calls=None, tool_call_id=None)]
INFO:     127.0.0.1:34050 - "POST /v1/chat/stream HTTP/1.1" 200 OK
2025-08-12 11:46:05,415 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-08-12 11:46:17,189 - __main__ - INFO - --- REQUEST METRICS ---
2025-08-12 11:46:17,189 - __main__ - INFO - [Latency] End-to-End: 11.96 seconds
2025-08-12 11:46:17,189 - __main__ - INFO - [Throughput] Chunks per Second: 110.50
2025-08-12 11:46:17,189 - __main__ - INFO - [Throughput] Total Chunks: 1322
2025-08-12 11:46:17,189 - __main__ - INFO - [Resources] Start - CPU: 6.9%, RAM: 14.6%
2025-08-12 11:46:17,189 - __main__ - INFO - [Resources] End   - CPU: 7.1%, RAM: 14.8%
2025-08-12 11:46:17,189 - __main__ - INFO - -----------------------
